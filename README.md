The project is a simulation of “LinkedIn”, one of the leading companies and the best platform for connecting applicants and recruiters across the world and across different domains of employment.

The project modules are classified under two types of users: An applicant and a recruiter. An applicant needs to have a valid registered account and a password to proceed with the offered set of functionalities. Otherwise, a user needs to first register as an Applicant. Similarly, a recruiter with valid recruiter credentials, can post various openings for jobs in his company. Applicants have an option of filtering the kind of jobs or opportunities they are looking for before searching for a job. These jobs are posted by recruiters.

An applicant can click on a job of their choice which would redirect the user to the Job Description of that job and provide an option to apply/easy apply for a job. Some applications have an option of “Easy Apply” where the applicant needn’t fill his details for that application since the data is collected from the applicant’s profile.

The first step involved was identifying the entities and comparing if they could be counted as objects. In this context, broadly speaking, Users(Applicant/Recruiter) is an object in our application. ‘Users’ has attributes like an email, password and name which can be changed or updated. Even after the changes are made, the object(Applicant/Recruiter) still retains its identity, thus making mutable. Managing objects involves handling requests, maintaining user entries in the database, fetching and updating the values of objects. This inter management of objects is usually distributed in different layers.
For instance, while searching for a job, we have the following entities: applicant, recruiter, job. The details of jobs as well as user details can be modified by the user. This acts as the isolated layer of our design which is used by the application layer to defines scenarios about the actions that can be performed by the existing entities.

The behavior of any application is decided by the Application Layer. The Application layer is primarily responsible for managing the interactions amongst the domains. For instance, here, an applicant when applying for a job, creates an entity application which truly starts holding a value when an applicant submits it. But the application undergoes several changes before it is finally submitted by an applicant. These temporary changes are not recorded in the database. But in another instance, if a user wants to apply again to the same job, the applicant would have to start the entire filing from the beginning and again the final entry to the database would be made on clicking submit. This basis of deciding when application behaves as an object and when it behaves like entity, like the one done in the case of which data to write to a database, is the extent to which the task had been performed (whether the submit button was pressed or not). However, in our case, an application cannot be removed from the database hence it cannot be reverted.

The bottom most layer is usually the infrastructure layer consisting of the database architecture containing all the entries. During user sign up, updating, application submission etc., the data gets stored in the database by creating different database objects (User, job, application etc.)


Since, the real time LinkedIn application runs on a large scale and tackles thousands of visits on the website daily, the architecture of the simulation was very important. To emphasize on a distributed architecture, we divided the entire functioning of the software into a 3-tiered architecture.
The first Tier of this architecture was the Client Side. The Client side involves all the entities and features that are visible and accessible to a user. In our case, the Login page, the Applicant/Recruiter Dashboard, the Job Search Page, all these components come under the client tier. The Client Tier is broadly classified as the Applicant Module and as the Recruiter Module each with their own sign ups, job posting/viewing and dashboards.
The second tier is the first part of the server tier where all the functionalities defined in the front end are implemented. The request for a service received from the Client Side like different REST API calls are made from here which acts as the middleware between the frontend and the database (source/destination based on type of call. From the API call at the Backend of the application, a request for a topic is made to the Kafka Backend.
Kafka is an asynchronous message queue designed to handle the vast amount of traffic on data coming to an application/software. The queue is maintained by defining topics and partitions among topics, the topics being specific to a kind of task that needs to be implemented. Therefore, Kafka is the primary heavy weight resource management concept that we have implemented in the application. By deviating and redirecting the traffic received from the client side based on their type, Kafka largely improves the performance time and reduces the costing of the application.
 
 Therefore, the Kafka backend is responsible for segregating the actions to be performed or the response to be sent to the client in case of a successful or failed call to the Third Tier which is the Database Tier. The database queries are therefore made from the Kafka Backend. Since Kafka is a Queue, it stores the request coming from the client or the response coming from the backend. These requests are attended to by the Backend in a FIFO fashion when the server is running. However, consumers(Backend) and producers(Frontend) both can subscribe to a topic for receiving or responding with a message. Because of this separation of functionalities, the application can hold under heavy actions.
The final and the most important tier of our Client-Middleware-Server Architecture is the Database Layer. All the operations that are being performed in Tier 2 or all the attributes or values being either displayed to collected from components in Tier 1 are stored, updated, fetched and manipulated in the databases. Database querying is an expensive and resource consuming job which involves proper segregation of data based on its use in the application. We have classified the entire data of the application into 2 types of databases: MySQL and MongoDB.

MongoDB is a No-SQL Column based database management system while MySQL is a Row based Relational Database Management System. MySQL database is used for:
Storing and Fetching the login details of both the kinds of users(Applicants/ Recruiters)
Fetching data of Applications except the media for an easy access to the recruiter to the applicant’s profile.
Fetching data of the job an applicant has applied to
Rest all the data like User information, Job Information and the entire Application is being stored in MongoDB. This distribution of data is another concept that has been followed in our application for heavy load balancing.
The handling of data like images and files are stored in S3 bucket which stores these resources on aws cloud storage. This removes the load of these resources off the backend servers and improves efficiency.

MySQL implements a concept of row-locking, which is a locking mechanism which declines the simultaneous updates or fetches when an action is being performed on that table. It also prevents collisions as MySQL ensures that queries are executed only when the previous query has been executed. Even a read operation cannot be performed when a write operation is being performed on a table thus making it a secure choice for storing login details with the required hashing functions (JWT+Passport in our case). But since, MySQL is a row-based database management system, it is slower as every column in a row is read once a query is run. This issue with speed is resolved my using No-SQL column-based database systems like MongoDB.

MongoDB also supports simultaneous read/write actions on the same Collection as the write requests sent to MongoDB server are queued and these requests are processed one by one. MongoDB also uses a technique called ‘instance-wide-locking' which has now been shifted to ‘database-wide lock’, in which MongoDB processes one query per database, but is able to use multiple databases. In terms of performance as well, MongoDB is faster and when placed under heavy load situations like our applications, MongoDB Server runs better than MySQL.

The final concept, that we implemented to ensure proper Heavy Resource Weight Management is SQL caching using Redis. Sometimes, distributed applications, no matter how hard we try to improve the efficiency of the application, often comes with a bottleneck where the performance of the application is directly impacted. For example, MongoDB really slows down when the application starts scaling up. If the database is detached from the server, the pace slows down even more. That is where caching comes in handy which is used to store recently accessed data for faster search. When used Redis caching between our mongo server and application server, we improve the efficiency of the application since the data is first looked for in the cache before visiting the database for retrieving it.
